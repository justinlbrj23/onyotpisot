/**
 * Universal ArcGIS FeatureServer Scraper
 * --------------------------------------
 * ‚úî Accepts ANY ArcGIS item data URL
 * ‚úî Auto‚Äëdiscovers FeatureServer + Layer ID
 * ‚úî Dynamically fetches available fields
 * ‚úî Writes all attributes to Google Sheets
 * ‚úî Batches writes to avoid cell limits
 * ‚úî Retries failed fetches with backoff
 */

import fetch from "node-fetch";
import { google } from "googleapis";

// --------------------------------------
// CONFIG
// --------------------------------------
const SHEET_ID = "1woBGXySPNxQavq_3FMsNK4WnqhJ4w_Oh5Ui_05rzzL0";
const SHEET_NAME = "Sheet1";

const ITEM_DATA_URL =
  "https://www.arcgis.com/sharing/rest/content/items/240d1eab8fa44e5e8baf244fb1a15365/data";

let SERVICE_ROOT = "";
let LAYER_ID = 0;
let ENDPOINT = "";
let METADATA_URL = "";

const TEST_SIZE = 10;
const PAGE_SIZE = 500;
const MAX_ROWS = 5000;
const MAX_RETRIES = 3;
const BATCH_SIZE = 500;

const ARCGIS_TOKEN = process.env.ARCGIS_TOKEN || "";

// --------------------------------------
// GOOGLE AUTH
// --------------------------------------
const auth = new google.auth.GoogleAuth({
  keyFile: "./service-account.json",
  scopes: ["https://www.googleapis.com/auth/spreadsheets"]
});
const sheets = google.sheets({ version: "v4", auth });

// --------------------------------------
// RETRY WRAPPER
// --------------------------------------
async function fetchWithRetry(url, options = {}, retries = MAX_RETRIES) {
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      const res = await fetch(url, options);
      if (!res.ok) throw new Error(`HTTP ${res.status}`);
      return await res.json();
    } catch (err) {
      console.warn(`‚ö†Ô∏è Fetch attempt ${attempt} failed: ${err.message}`);
      if (attempt === retries) throw err;
      await new Promise(r => setTimeout(r, 1000 * attempt));
    }
  }
}

// --------------------------------------
// DISCOVER FEATURESERVER FROM ITEM DATA
// --------------------------------------
async function resolveFeatureServer() {
  const data = await fetchWithRetry(`${ITEM_DATA_URL}?f=json`);

  // Case 1: Feature Collection
  if (data.layers?.length && data.layers[0].url) {
    SERVICE_ROOT = data.layers[0].url.replace(/\/\d+$/, "");
    LAYER_ID = parseInt(data.layers[0].url.split("/").pop(), 10);
  }

  // Case 2: WebMap referencing external services
  else if (data.operationalLayers?.length && data.operationalLayers[0].url) {
    SERVICE_ROOT = data.operationalLayers[0].url.replace(/\/\d+$/, "");
    LAYER_ID = parseInt(data.operationalLayers[0].url.split("/").pop(), 10);
  }

  else {
    throw new Error("‚ùå Could not locate FeatureServer URL inside item data.");
  }

  ENDPOINT = `${SERVICE_ROOT}/${LAYER_ID}/query`;
  METADATA_URL = `${SERVICE_ROOT}/${LAYER_ID}?f=pjson`;

  console.log("üîó Resolved FeatureServer:", SERVICE_ROOT);
  console.log("üî¢ Layer ID:", LAYER_ID);
}

// --------------------------------------
// GET AVAILABLE FIELDS
// --------------------------------------
async function getAvailableFields() {
  const url = ARCGIS_TOKEN ? `${METADATA_URL}&token=${ARCGIS_TOKEN}` : METADATA_URL;
  const meta = await fetchWithRetry(url);

  if (!meta.fields) {
    console.error("‚ö†Ô∏è No fields array found in metadata. Raw metadata:", meta);
    return [];
  }

  const fields = meta.fields.map(f => f.name);
  console.log("üìë Available fields:", fields);
  return fields;
}

// --------------------------------------
// FETCH PAGE
// --------------------------------------
async function fetchPage(offset, outFields, size) {
  const params = new URLSearchParams({
    where: "1=1",
    outFields: outFields.length ? outFields.join(",") : "*",
    returnGeometry: "false",
    f: "json",
    resultOffset: offset,
    resultRecordCount: size
  });

  if (ARCGIS_TOKEN) params.append("token", ARCGIS_TOKEN);

  return await fetchWithRetry(`${ENDPOINT}?${params}`);
}

// --------------------------------------
// SHEET HELPERS
// --------------------------------------
async function clearSheet() {
  await sheets.spreadsheets.values.clear({
    spreadsheetId: SHEET_ID,
    range: SHEET_NAME
  });
}

async function writeHeaders(headers) {
  await sheets.spreadsheets.values.update({
    spreadsheetId: SHEET_ID,
    range: `${SHEET_NAME}!A1`,
    valueInputOption: "RAW",
    requestBody: { values: [headers] }
  });
}

async function appendRowsBatch(rows) {
  await sheets.spreadsheets.values.append({
    spreadsheetId: SHEET_ID,
    range: `${SHEET_NAME}!A2`,
    valueInputOption: "RAW",
    insertDataOption: "INSERT_ROWS",
    requestBody: { values: rows }
  });
}

// --------------------------------------
// MAIN
// --------------------------------------
async function run() {
  console.log("üîé Resolving FeatureServer from ArcGIS item...");
  await resolveFeatureServer();

  const fields = await getAvailableFields();

  console.log("üîé Testing ArcGIS with 10 records...");
  const testData = await fetchPage(0, fields, TEST_SIZE);

  if (!testData.features?.length) {
    console.log("‚ö†Ô∏è Test query returned no features.");
    return;
  }

  console.log("üîç Sample record keys:", Object.keys(testData.features[0].attributes));
  console.log("üîç Sample record values:", testData.features[0].attributes);

  let offset = 0;
  let hasMore = true;
  let rowsRaw = [];

  console.log("üîé Fetching parcels from ArcGIS (bulk)...");

  while (hasMore && rowsRaw.length < MAX_ROWS) {
    const data = await fetchPage(offset, fields, PAGE_SIZE);

    if (!data.features?.length) {
      console.log("‚ö†Ô∏è No features returned at offset", offset);
      break;
    }

    rowsRaw.push(...data.features.map(f => f.attributes));

    offset += PAGE_SIZE;
    hasMore = data.exceededTransferLimit === true;
  }

  if (rowsRaw.length > MAX_ROWS) {
    rowsRaw = rowsRaw.slice(0, MAX_ROWS);
  }

  console.log(`üì¶ Total records: ${rowsRaw.length}`);
  console.log(`üìä Total cells to write: ${rowsRaw.length * fields.length}`);

  await clearSheet();
  await writeHeaders(fields);

  const rows = rowsRaw.map(p => fields.map(field => p[field] ?? ""));

  if (!rows.length) {
    console.log("‚úÖ No rows to write");
    return;
  }

  for (let i = 0; i < rows.length; i += BATCH_SIZE) {
    const batch = rows.slice(i, i + BATCH_SIZE);
    await appendRowsBatch(batch);
    console.log(`‚úÖ Appended batch ${i / BATCH_SIZE + 1} (${batch.length} rows)`);
  }

  console.log(`‚úÖ Wrote ${rows.length} rows to Google Sheets in batches`);
}

// --------------------------------------
run().catch(err => {
  console.error("‚ùå Error:", err);
  process.exit(1);
});